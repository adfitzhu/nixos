{ config, pkgs, lib, ... }:

{
  # Local AI bundle with Intel Arc A770 GPU acceleration via IPEX-LLM
  # Architecture:
  #   - IPEX-LLM Ollama (Docker): GPU-accelerated inference on Intel Arc
  #   - Open WebUI: ChatGPT-like web interface
  #   - LiteLLM: Unified OpenAI-compatible API

  # Disable system Ollama - we use the IPEX-LLM Docker version instead
  # services.ollama.enable = false;

  # Docker containers for AI stack
  virtualisation.docker.enable = true;
  virtualisation.oci-containers = {
    backend = "docker";
    containers = {
      # IPEX-LLM Ollama with Intel Arc GPU acceleration
      ipex-ollama = {
        image = "intelanalytics/ipex-llm-inference-cpp-xpu:latest";
        autoStart = true;
        volumes = [
          "ollama-models:/root/.ollama"
        ];
        environment = {
          DEVICE = "Arc";
          OLLAMA_HOST = "0.0.0.0:11434";
          OLLAMA_NUM_PARALLEL = "2";
          OLLAMA_MAX_LOADED_MODELS = "1";
          # Use only the Arc A770 (device 0), not the integrated GPU
          ONEAPI_DEVICE_SELECTOR = "level_zero:0";
          # Performance tuning for Arc
          SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS = "1";
          # Enable memory info
          ZES_ENABLE_SYSMAN = "1";
        };
        extraOptions = [
          "--device=/dev/dri"
          "--network=host"
          "--shm-size=16g"
          "--privileged"  # Needed for GPU access
        ];
        # The container has its own entrypoint that sets up oneAPI
        cmd = [ "bash" "-c" "cd /llm/scripts && ./start-ollama.sh; sleep infinity" ];
      };

      # Open WebUI - ChatGPT-like interface
      open-webui = {
        image = "ghcr.io/open-webui/open-webui:main";
        autoStart = true;
        volumes = [
          "open-webui:/app/backend/data"
        ];
        dependsOn = [ "ipex-ollama" ];
        extraOptions = [
          "--network=host"  # Use host networking to reach Ollama
        ];
        environment = {
          PORT = "3000";  # Run on port 3000 since we're using host networking
          OLLAMA_BASE_URL = "http://127.0.0.1:11434";
          OPENAI_API_BASE_URLS = "http://127.0.0.1:4000/v1";
          ENABLE_OPENAI_API = "true";
          ENABLE_MODEL_FILTER = "false";
          # Disable authentication (for home/local use only)
          WEBUI_AUTH = "false";
          # Audio: Use Speaches for TTS (OpenAI-compatible)
          AUDIO_TTS_ENGINE = "openai";
          AUDIO_TTS_OPENAI_API_BASE_URL = "http://127.0.0.1:8000/v1";
          AUDIO_TTS_OPENAI_API_KEY = "not-needed";
          AUDIO_TTS_MODEL = "speaches-ai/Kokoro-82M-v1.0-ONNX";  # Full model ID required
          AUDIO_TTS_VOICE = "af_heart";  # Kokoro voice (natural female)
          # Audio: Use Speaches for STT (OpenAI-compatible)
          AUDIO_STT_ENGINE = "openai";
          AUDIO_STT_OPENAI_API_BASE_URL = "http://127.0.0.1:8000/v1";
          AUDIO_STT_OPENAI_API_KEY = "not-needed";
          AUDIO_STT_MODEL = "Systran/faster-whisper-small.en";  # Full model ID required
        };
      };

      # LiteLLM proxy - OpenAI-compatible API aggregator
      litellm-proxy = {
        image = "ghcr.io/berriai/litellm:main-latest";
        autoStart = true;
        volumes = [
          "/var/lib/litellm:/app/config"
        ];
        dependsOn = [ "ipex-ollama" ];
        extraOptions = [
          "--network=host"  # Use host networking to reach Ollama
        ];
        environment = {
          LITELLM_MASTER_KEY = "sk-local-ai-key";
          PORT = "4000";
        };
        cmd = [ "--config" "/app/config/config.yaml" "--port" "4000" ];
      };

      # Speaches - Combined TTS + STT server (OpenAI-compatible)
      # Includes Piper TTS, Kokoro TTS (#1 ranked), and Faster-Whisper STT
      # Access at: http://localhost:8000
      # Note: CPU-only for now - Whisper has no Intel GPU support in Docker yet
      # Piper TTS is designed for CPU and is very fast anyway
      speaches = {
        image = "ghcr.io/speaches-ai/speaches:latest-cpu";
        autoStart = true;
        volumes = [
          "speaches-cache:/home/ubuntu/.cache/huggingface/hub"
        ];
        extraOptions = [
          "--network=host"
        ];
        environment = {
          # Use environment variables for config (double underscore for nested)
          UVICORN_HOST = "0.0.0.0";
          UVICORN_PORT = "8000";
          # Connect to local Ollama for voice chat mode
          CHAT_COMPLETION_BASE_URL = "http://127.0.0.1:11434/v1";
          # Model settings
          STT_MODEL_TTL = "300";  # Unload after 5 min idle
          TTS_MODEL_TTL = "300";
          # Enable VAD for better transcription
          _UNSTABLE_VAD_FILTER = "true";
          LOG_LEVEL = "info";
        };
      };
    };
  };

  # Intel Arc GPU compute stack
  hardware.graphics = {
    enable = true;
    enable32Bit = true;
    extraPackages = with pkgs; [
      intel-compute-runtime
      intel-media-driver
      vpl-gpu-rt
      level-zero
      intel-gmmlib
      openvino
    ];
  };

  # Ensure render group exists
  users.groups.render = {};

  # System packages and helper tools
  environment.systemPackages = with pkgs; [
    intel-gpu-tools  # Provides intel_gpu_top
    nvtopPackages.intel  # Alternative GPU monitor
    clinfo
    vulkan-tools

    # Main setup script
    (writeShellScriptBin "ai-setup" ''
      #!/usr/bin/env bash
      echo "ðŸ¤– Intel Arc Local AI Setup (IPEX-LLM Docker)"
      echo "=============================================="
      echo ""
      
      # Create directories
      sudo mkdir -p /var/lib/litellm
      sudo chown -R $USER:users /var/lib/litellm
      
      # Create LiteLLM config
      cat > /var/lib/litellm/config.yaml << 'EOF'
model_list:
  # Ollama models via IPEX-LLM (GPU accelerated)
  - model_name: "ollama/*"
    litellm_params:
      model: "ollama/*"
      api_base: http://127.0.0.1:11434

  # Common model aliases
  - model_name: llama3.2:3b
    litellm_params:
      model: ollama/llama3.2:3b
      api_base: http://127.0.0.1:11434

  - model_name: llama3.2
    litellm_params:
      model: ollama/llama3.2
      api_base: http://127.0.0.1:11434
      
  - model_name: deepseek-r1:7b
    litellm_params:
      model: ollama/deepseek-r1:7b
      api_base: http://127.0.0.1:11434

  - model_name: codellama
    litellm_params:
      model: ollama/codellama
      api_base: http://127.0.0.1:11434

  - model_name: qwen2.5-coder
    litellm_params:
      model: ollama/qwen2.5-coder
      api_base: http://127.0.0.1:11434

general_settings:
  master_key: sk-local-ai-key
EOF

      echo "âœ… Configuration created!"
      echo ""
      echo "ðŸŒ Access Points:"
      echo "  Web UI:      http://localhost:3000"
      echo "  Ollama API:  http://localhost:11434 (GPU accelerated!)"
      echo "  OpenAI API:  http://localhost:4000/v1"
      echo ""
      echo "ðŸ“š Quick Start:"
      echo "  1. Pull a model:  ai-pull llama3.2:3b"
      echo "  2. Check status:  ai-status"
      echo "  3. Open Web UI:   firefox http://localhost:3000"
      echo "  4. See models:    ai-models"
    '')

    # Pull model via Docker (since ollama CLI isn't installed on host)
    (writeShellScriptBin "ai-pull" ''
      #!/usr/bin/env bash
      if [ -z "$1" ]; then
        echo "Usage: ai-pull <model-name>"
        echo "Example: ai-pull llama3.2:3b"
        echo ""
        echo "Run 'ai-models' for recommended models"
        exit 1
      fi
      
      echo "ðŸ“¥ Pulling model: $1"
      echo ""
      docker exec -it ipex-ollama /llm/ollama/ollama pull "$1"
    '')

    # Run model interactively via Docker
    (writeShellScriptBin "ai-run" ''
      #!/usr/bin/env bash
      if [ -z "$1" ]; then
        echo "Usage: ai-run <model-name>"
        echo "Example: ai-run llama3.2:3b"
        exit 1
      fi
      
      echo "ðŸ¤– Running model: $1 (Intel Arc GPU)"
      echo "Type 'exit' or Ctrl+D to quit"
      echo ""
      docker exec -it ipex-ollama /llm/ollama/ollama run "$1"
    '')

    # List models via Docker
    (writeShellScriptBin "ai-list" ''
      #!/usr/bin/env bash
      echo "ðŸ“‹ Installed Models:"
      docker exec ipex-ollama /llm/ollama/ollama list
    '')

    # Model recommendations for Arc A770
    (writeShellScriptBin "ai-models" ''
      #!/usr/bin/env bash
      echo "ðŸ¤– Recommended Models for Intel Arc A770 (16GB VRAM)"
      echo "===================================================="
      echo ""
      echo "ðŸ“Š Model Sizes & Performance:"
      echo ""
      echo "ðŸŸ¢ SMALL (Fast, <8GB VRAM):"
      echo "  ai-pull llama3.2:3b         # General purpose, very fast"
      echo "  ai-pull phi4                # Microsoft, efficient"
      echo "  ai-pull qwen2.5-coder:3b    # Coding focused"
      echo "  ai-pull gemma2:2b           # Google, compact"
      echo ""
      echo "ðŸŸ¡ MEDIUM (Balanced, 8-14GB VRAM):"
      echo "  ai-pull deepseek-r1:7b      # Reasoning model (recommended!)"
      echo "  ai-pull llama3.2:7b         # Better quality"
      echo "  ai-pull codellama:7b        # Code generation"
      echo "  ai-pull mistral:7b          # Fast & capable"
      echo "  ai-pull qwen2.5:7b          # Multilingual"
      echo ""
      echo "ðŸ”´ LARGE (Best quality, 14-16GB VRAM):"
      echo "  ai-pull llama3.2:13b        # High quality"
      echo "  ai-pull codellama:13b       # Best for code"
      echo ""
      echo "ðŸ’¡ Tips:"
      echo "  â€¢ Start with 3b/7b models to test GPU acceleration"
      echo "  â€¢ Run 'ai-test-gpu' while chatting to see GPU usage"
      echo "  â€¢ Use 'ai-run <model>' for CLI chat"
      echo "  â€¢ Or open http://localhost:3000 for web UI"
    '')

    # Status check script
    (writeShellScriptBin "ai-status" ''
      #!/usr/bin/env bash
      echo "ðŸ” AI Stack Status"
      echo "=================="
      echo ""
      
      # Check IPEX-Ollama container
      if docker ps --format '{{.Names}}' | grep -q "ipex-ollama"; then
        echo "âœ… IPEX-Ollama: Running (Intel Arc GPU)"
        if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
          echo "   API: http://localhost:11434 âœ…"
          MODELS=$(docker exec ipex-ollama /llm/ollama/ollama list 2>/dev/null | tail -n +2)
          if [ -n "$MODELS" ]; then
            echo "   Models:"
            echo "$MODELS" | while read line; do echo "     $line"; done
          else
            echo "   âš ï¸  No models installed (run: ai-pull llama3.2:3b)"
          fi
        else
          echo "   API: Starting up... (wait a moment)"
        fi
      else
        echo "âŒ IPEX-Ollama: Not running"
        echo "   Try: sudo systemctl start docker-ipex-ollama"
      fi
      echo ""
      
      # Check Open WebUI
      if curl -s http://localhost:3000 >/dev/null 2>&1; then
        echo "âœ… Web UI:      Running (http://localhost:3000)"
      else
        echo "âŒ Web UI:      Not responding"
        echo "   Try: sudo systemctl start docker-open-webui"
      fi
      echo ""
      
      # Check LiteLLM
      if curl -s http://localhost:4000/health >/dev/null 2>&1; then
        echo "âœ… LiteLLM:     Running (http://localhost:4000)"
      else
        echo "âš ï¸  LiteLLM:     Not responding"
        echo "   Try: sudo systemctl start docker-litellm-proxy"
      fi
      echo ""
      
      # GPU Info
      echo "ðŸŽ® Intel GPU:"
      if [ -e /dev/dri/renderD128 ]; then
        echo "   Render device: /dev/dri/renderD128 âœ…"
        clinfo -l 2>/dev/null | grep -i "Arc\|intel" | head -3 | while read line; do echo "   $line"; done
      else
        echo "   âš ï¸  No render device found"
      fi
    '')

    # GPU test script
    (writeShellScriptBin "ai-test-gpu" ''
      #!/usr/bin/env bash
      echo "ðŸ§ª Intel GPU Test"
      echo "================="
      echo ""
      
      echo "ðŸ“Š GPU Devices:"
      ls -la /dev/dri/ 2>/dev/null
      echo ""
      
      echo "ðŸ”§ OpenCL Platforms:"
      clinfo -l 2>/dev/null || echo "clinfo not available"
      echo ""
      
      echo "ðŸ“ˆ GPU Activity:"
      echo "(requires sudo - press Ctrl+C to stop)"
      sudo intel_gpu_top -l 2>/dev/null || nvtop 2>/dev/null || echo "GPU monitoring not available"
    '')

    # API test script
    (writeShellScriptBin "ai-test-api" ''
      #!/usr/bin/env bash
      echo "ðŸ§ª Testing AI APIs"
      echo "=================="
      echo ""
      
      MODEL=''${1:-llama3.2:3b}
      
      echo "ðŸ“¡ Testing Ollama API with $MODEL..."
      RESPONSE=$(curl -s http://localhost:11434/api/generate -d "{
        \"model\": \"$MODEL\",
        \"prompt\": \"Say hello in one sentence.\",
        \"stream\": false
      }" 2>&1)
      
      if echo "$RESPONSE" | grep -q "response"; then
        echo "âœ… Ollama API working!"
        echo "Response: $(echo "$RESPONSE" | grep -o '"response":"[^"]*"' | head -1)"
      else
        echo "âŒ Ollama API error: $RESPONSE"
      fi
      echo ""
      
      echo "ðŸ“¡ Testing OpenAI-compatible API (LiteLLM)..."
      RESPONSE=$(curl -s http://localhost:4000/v1/chat/completions \
        -H "Authorization: Bearer sk-local-ai-key" \
        -H "Content-Type: application/json" \
        -d "{
          \"model\": \"$MODEL\",
          \"messages\": [{\"role\": \"user\", \"content\": \"Say hello briefly.\"}]
        }" 2>&1)
      
      if echo "$RESPONSE" | grep -q "choices"; then
        echo "âœ… OpenAI API working!"
      else
        echo "âš ï¸  OpenAI API: $RESPONSE"
      fi
    '')

    # View IPEX-Ollama logs
    (writeShellScriptBin "ai-logs" ''
      #!/usr/bin/env bash
      echo "ðŸ“œ IPEX-Ollama Container Logs"
      echo "============================="
      docker logs -f ipex-ollama
    '')

    # Restart AI stack
    (writeShellScriptBin "ai-restart" ''
      #!/usr/bin/env bash
      echo "ðŸ”„ Restarting AI Stack..."
      sudo systemctl restart docker-ipex-ollama
      sleep 5
      sudo systemctl restart docker-open-webui
      sudo systemctl restart docker-litellm-proxy
      echo "âœ… Done! Run 'ai-status' to check."
    '')

    # Voice chat status
    (writeShellScriptBin "ai-voice-status" ''
      #!/usr/bin/env bash
      echo "ðŸŽ¤ Voice Chat Status (Speaches)"
      echo "================================"
      echo ""
      
      # Check Speaches
      if curl -s http://localhost:8000/health >/dev/null 2>&1; then
        echo "âœ… Speaches:  Running (http://localhost:8000)"
        echo ""
        
        # Check loaded models
        MODELS=$(curl -s http://localhost:8000/v1/models 2>&1)
        if echo "$MODELS" | grep -q "Kokoro"; then
          echo "   TTS Model: speaches-ai/Kokoro-82M-v1.0-ONNX âœ…"
        else
          echo "   TTS Model: Not loaded âš ï¸  (run: ai-voice-setup)"
        fi
        if echo "$MODELS" | grep -q "whisper"; then
          echo "   STT Model: Systran/faster-whisper-small.en âœ…"
        else
          echo "   STT Model: Not loaded âš ï¸  (run: ai-voice-setup)"
        fi
        echo ""
        echo "   WebUI: http://localhost:8000 (built-in test interface)"
      elif docker ps --format '{{.Names}}' | grep -q "speaches"; then
        echo "â³ Speaches:  Starting up..."
        echo "   Check progress: ai-voice-logs"
      else
        echo "âŒ Speaches:  Not running"
        echo "   Try: sudo systemctl start docker-speaches"
      fi
      echo ""
      
      echo "ðŸ’¡ Usage:"
      echo "   1. Run: ai-voice-setup (first time, downloads models)"
      echo "   2. Open http://localhost:3000"
      echo "   3. Click the microphone icon to speak"
      echo "   4. AI will respond with voice!"
      echo ""
      echo "ðŸŽ­ Voice Options (change in Open WebUI Settings â†’ Audio):"
      echo "   Kokoro voices: af_heart, af_bella, am_adam, am_michael"
      echo ""
      echo "âš ï¸  Note: Voice runs on CPU (no Intel GPU Whisper support yet)"
      echo "   Kokoro TTS is very fast and high quality on CPU"
    '')

    # Setup voice models (download on first use)
    (writeShellScriptBin "ai-voice-setup" ''
      #!/usr/bin/env bash
      echo "ðŸŽ¤ Setting up Voice Models (Speaches)"
      echo "======================================"
      echo ""
      
      if ! curl -s http://localhost:8000/health >/dev/null 2>&1; then
        echo "âŒ Speaches not running! Start it first:"
        echo "   sudo systemctl start docker-speaches"
        exit 1
      fi
      
      echo "ðŸ“¥ Downloading Kokoro TTS model (~180MB)..."
      curl -X POST "http://localhost:8000/v1/models/speaches-ai%2FKokoro-82M-v1.0-ONNX"
      echo ""
      
      echo "ðŸ“¥ Downloading Whisper STT model (~500MB)..."
      curl -X POST "http://localhost:8000/v1/models/Systran%2Ffaster-whisper-small.en"
      echo ""
      
      echo "âœ… Voice models installed!"
      echo ""
      echo "Test with: ai-voice-test"
      echo "Or open: http://localhost:3000"
    '')

    # Test TTS
    (writeShellScriptBin "ai-voice-test" ''
      #!/usr/bin/env bash
      echo "ðŸ§ª Testing Voice Services (Speaches)"
      echo "====================================="
      echo ""
      
      # First check if models are loaded
      MODELS=$(curl -s http://localhost:8000/v1/models 2>&1)
      if echo "$MODELS" | grep -q '"data":\[\]'; then
        echo "âš ï¸  No models loaded! Downloading required models..."
        echo ""
        echo "ðŸ“¥ Downloading Kokoro TTS model..."
        curl -s -X POST "http://localhost:8000/v1/models/speaches-ai%2FKokoro-82M-v1.0-ONNX" >/dev/null 2>&1
        echo "ðŸ“¥ Downloading Whisper STT model..."
        curl -s -X POST "http://localhost:8000/v1/models/Systran%2Ffaster-whisper-small.en" >/dev/null 2>&1
        echo "âœ… Models downloaded!"
        echo ""
      fi
      
      echo "ðŸ“¢ Testing Text-to-Speech (Kokoro)..."
      RESPONSE=$(curl -s -X POST http://localhost:8000/v1/audio/speech \
        -H "Content-Type: application/json" \
        -d '{"model": "speaches-ai/Kokoro-82M-v1.0-ONNX", "voice": "af_heart", "input": "Hello! Voice chat is working great."}' \
        --output /tmp/test-tts.mp3 -w "%{http_code}" 2>&1)
      
      if [ "$RESPONSE" = "200" ] && [ -s /tmp/test-tts.mp3 ]; then
        echo "âœ… TTS working! Playing audio..."
        ${pkgs.mpv}/bin/mpv --no-video /tmp/test-tts.mp3 2>/dev/null || \
          echo "   Audio saved to /tmp/test-tts.mp3 (install mpv to play)"
      else
        echo "âŒ TTS error (HTTP $RESPONSE)"
        echo "   Check: ai-voice-logs"
      fi
      echo ""
      
      echo "ðŸŽ¤ Testing Speech-to-Text (Whisper)..."
      if [ -f /tmp/test-tts.mp3 ]; then
        RESPONSE=$(curl -s -X POST http://localhost:8000/v1/audio/transcriptions \
          -F "file=@/tmp/test-tts.mp3" \
          -F "model=Systran/faster-whisper-small.en" 2>&1)
        
        if echo "$RESPONSE" | grep -q "text"; then
          echo "âœ… STT working!"
          echo "   Transcribed: $(echo "$RESPONSE" | grep -o '"text":"[^"]*"' | sed 's/"text":"//;s/"$//')"
        else
          echo "âš ï¸  STT response: $RESPONSE"
        fi
      else
        echo "âš ï¸  No test audio file (TTS must work first)"
      fi
      echo ""
      
      echo "ðŸŽ™ï¸  Voice chat ready! Open http://localhost:3000"
    '')

    # Voice logs
    (writeShellScriptBin "ai-voice-logs" ''
      #!/usr/bin/env bash
      echo "ðŸ“œ Speaches Logs"
      echo "================"
      docker logs -f speaches
    '')

    # Restart voice services
    (writeShellScriptBin "ai-voice-restart" ''
      #!/usr/bin/env bash
      echo "ðŸ”„ Restarting Speaches..."
      sudo systemctl restart docker-speaches
      echo "âœ… Done! Run 'ai-voice-status' to check."
    '')
  ];

  # Firewall configuration
  networking.firewall.allowedTCPPorts = [
    11434  # Ollama API
    3000   # Open WebUI
    4000   # LiteLLM (OpenAI-compatible)
    8000   # Speaches TTS/STT API
  ];

  # GPU permissions setup service
  systemd.services.ai-gpu-setup = {
    description = "Setup Intel GPU for AI workloads";
    before = [ "docker-ipex-ollama.service" ];
    wantedBy = [ "multi-user.target" ];
    serviceConfig = {
      Type = "oneshot";
      RemainAfterExit = true;
    };
    script = ''
      ${pkgs.coreutils}/bin/chmod 666 /dev/dri/renderD* 2>/dev/null || true
      ${pkgs.coreutils}/bin/chmod 666 /dev/dri/card* 2>/dev/null || true
      echo "Intel GPU permissions configured"
    '';
  };

  # System optimizations for AI
  boot.kernel.sysctl = {
    "kernel.shmmax" = 68719476736;
    "kernel.shmall" = 4294967296;
  };
}
